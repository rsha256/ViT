{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Fill in model architecture (maybe ask them to implement attention by hand? might be too hard)\n",
    "- Load pretrained weights\n",
    "- Load ImageNet\n",
    "- Fine-tune on ImageNet (write training loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformer architecture\n",
    "\n",
    "First, go to **vit_architecture.py** and fill in some model definitions. You might find this illustration of the architecture of a vision transformer helpful (Figure 1 from the [ViT paper](https://arxiv.org/pdf/2010.11929.pdf)).\n",
    "\n",
    "![](vit_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete all the TODOs, run the cell below to import the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "    # attributes\n",
       "    num_classes = 10\n",
       "    patch_size = 16\n",
       "    hidden_size = 768\n",
       "    transformer_config = {'mlp_dim': 3072, 'num_heads': 12, 'num_layers': 12, 'attention_dropout_rate': 0.0, 'dropout_rate': 0.0}\n",
       "    cls_head_bias_init = 0.0\n",
       "    model_name = 'ViT-S_16'\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vit_architecture_solutions import VisionTransformer\n",
    "# from vit_architecture import VisionTransformer\n",
    "\n",
    "config = {\n",
    "    \"num_classes\": 10,\n",
    "    \"patch_size\": 16,\n",
    "    \"hidden_size\": 768,\n",
    "    \"model_name\": 'ViT-S_16',\n",
    "}\n",
    "\n",
    "transformer_config = {\n",
    "    \"mlp_dim\": 3072,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"attention_dropout_rate\": 0.0,\n",
    "    \"dropout_rate\": 0.0,\n",
    "}\n",
    "\n",
    "model = VisionTransformer(\n",
    "    num_classes=config[\"num_classes\"], \n",
    "    patch_size=config[\"patch_size\"],\n",
    "    hidden_size=config[\"hidden_size\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    transformer_config=transformer_config,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 06:33:38.968376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-05 06:33:38.968447: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-05 06:33:38.968458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ubuntu/anaconda3/envs/cs182/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from vit_utils import load_pretrained\n",
    "jax.local_devices() # shows available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained S_16 checkpoint\n",
    "checkpoint_file = \"S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz\"\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.system(\"mkdir checkpoints\")\n",
    "os.chdir(\"checkpoints\")\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    os.system(f\"wget https://storage.googleapis.com/vit_models/augreg/{checkpoint_file}\")\n",
    "os.chdir(\"..\")\n",
    "with open(f\"checkpoints/{checkpoint_file}\", \"rb\") as f:\n",
    "    ckpt_dict = np.load(f, allow_pickle=False)\n",
    "    keys, values = zip(*list(ckpt_dict.items()))\n",
    "\n",
    "# Initialize the model with random parameters\n",
    "variables = jax.jit(lambda: model.init(\n",
    "    jax.random.PRNGKey(0),\n",
    "    np.random.randn(2, 224, 224, 3),\n",
    "    train=False,\n",
    "), backend='cpu')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained: resized variant: %s to %s (1, 197, 384) (1, 197, 768)\n",
      "load_pretrained: grid-size from %s to %s 14 14\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "params = load_pretrained(\n",
    "    pretrained_path=f'checkpoints/{checkpoint_file}',\n",
    "    init_params=variables['params'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape=torch.Size([3, 32, 32])\n",
      "label=3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from vit_utils import get_cifar10, NumpyLoader, one_hot, accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "n_targets = 10\n",
    "batch_size = 128\n",
    "cifar10_train, cifar10_test = get_cifar10()\n",
    "# cifar10_train = NumpyLoader(cifar10_train, batch_size=batch_size, num_workers=0)\n",
    "# cifar10_test = NumpyLoader(cifar10_test, batch_size=batch_size, num_workers=0)\n",
    "# batch = next(iter(cifar10_test))\n",
    "for image, label in cifar10_test:\n",
    "    print(f\"{image.shape=}\")\n",
    "    plt.imshow(einops.rearrange(image, \"c h w -> h w c\"))\n",
    "    print(f\"{label=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the full train dataset (for checking accuracy while training)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(cifar10_train\u001b[39m.\u001b[39;49mtrain_data)\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(cifar10_train\u001b[39m.\u001b[39mtrain_data), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_labels \u001b[39m=\u001b[39m one_hot(np\u001b[39m.\u001b[39marray(cifar10_train\u001b[39m.\u001b[39mtrain_labels), n_targets)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Get full test dataset\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = np.array(cifar10_train.train_data).reshape(len(cifar10_train.train_data), -1)\n",
    "train_labels = one_hot(np.array(cifar10_train.train_labels), n_targets)\n",
    "\n",
    "# Get full test dataset\n",
    "test_images = jnp.array(cifar10_test.test_data.numpy().reshape(len(cifar10_test.test_data), -1), dtype=jnp.float32)\n",
    "test_labels = one_hot(np.array(cifar10_test.test_labels), n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "CallCompactUnboundModuleError",
     "evalue": "Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m model(image\u001b[39m.\u001b[39;49mnumpy())\n",
      "File \u001b[0;32m~/anaconda3/envs/cs182/lib/python3.10/site-packages/flax/linen/module.py:411\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    410\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 411\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    412\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    413\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs182/lib/python3.10/site-packages/flax/linen/module.py:722\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m is_compact_method:\n\u001b[1;32m    721\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mCallCompactUnboundModuleError()\n\u001b[1;32m    723\u001b[0m   is_recurrent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_compact_method\n\u001b[1;32m    724\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_compact_method \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m: Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError)"
     ]
    }
   ],
   "source": [
    "out = model(image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.cls: DeviceArray (10,)\n",
      "params_repl.cls: ShardedDeviceArray (1, 10)\n"
     ]
    }
   ],
   "source": [
    "import flax\n",
    "\n",
    "# So far, all our data is in the host memory. Let's now replicate the arrays\n",
    "# into the devices.\n",
    "# This will make every array in the pytree params become a ShardedDeviceArray\n",
    "# that has the same data replicated across all local devices.\n",
    "# For TPU it replicates the params in every core.\n",
    "# For a single GPU this simply moves the data onto the device.\n",
    "# For CPU it simply creates a copy.\n",
    "params_repl = flax.jax_utils.replicate(params)\n",
    "print('params.cls:', type(params['head']['bias']).__name__,\n",
    "      params['head']['bias'].shape)\n",
    "print('params_repl.cls:', type(params_repl['head']['bias']).__name__,\n",
    "      params_repl['head']['bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then map the call to our model's forward pass onto all available devices.\n",
    "vit_apply_repl = jax.pmap(lambda params, inputs: model.apply(\n",
    "    dict(params=params), inputs, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy(params_repl, torch_loader: DataLoader):\n",
    "  \"\"\"Returns accuracy evaluated on the test set.\"\"\"\n",
    "  good = total = 0\n",
    "  for _, batch in zip(tqdm.trange(steps), ds_test.as_numpy_iterator()):\n",
    "  for image, label in torch_loader:\n",
    "    image = image.numpy()\n",
    "    predicted = vit_apply_repl(params_repl, image)\n",
    "    print(predicted.shape)\n",
    "    print(label.shape)\n",
    "    break\n",
    "    is_same = predicted.argmax(axis=-1) == label.argmax(axis=-1)\n",
    "    good += is_same.sum()\n",
    "    total += len(is_same.flatten())\n",
    "  return good #/ total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cs182')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ab0e2aa881ca40358338efa0b4935a3094aa552aa6f563c2ae8f08225d88408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
