{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Fill in model architecture (maybe ask them to implement attention by hand? might be too hard)\n",
    "- Load pretrained weights\n",
    "- Load ImageNet\n",
    "- Fine-tune on ImageNet (write training loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformer architecture\n",
    "\n",
    "First, go to **vit_architecture.py** and fill in some model definitions. You might find this illustration of the architecture of a vision transformer helpful (Figure 1 from the [ViT paper](https://arxiv.org/pdf/2010.11929.pdf)).\n",
    "\n",
    "![](vit_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete all the TODOs, run the cell below to import the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "    # attributes\n",
       "    num_classes = 10\n",
       "    patch_size = 16\n",
       "    hidden_size = 768\n",
       "    transformer_config = {'mlp_dim': 3072, 'num_heads': 12, 'num_layers': 12, 'attention_dropout_rate': 0.0, 'dropout_rate': 0.0}\n",
       "    cls_head_bias_init = 0.0\n",
       "    model_name = 'ViT-S_16'\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vit_architecture_solutions import VisionTransformer\n",
    "# from vit_architecture import VisionTransformer\n",
    "\n",
    "config = {\n",
    "    \"num_classes\": 10,\n",
    "    \"patch_size\": 16,\n",
    "    \"hidden_size\": 768,\n",
    "    \"model_name\": 'ViT-S_16',\n",
    "}\n",
    "\n",
    "transformer_config = {\n",
    "    \"mlp_dim\": 3072,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"attention_dropout_rate\": 0.0,\n",
    "    \"dropout_rate\": 0.0,\n",
    "}\n",
    "\n",
    "model = VisionTransformer(\n",
    "    num_classes=config[\"num_classes\"], \n",
    "    patch_size=config[\"patch_size\"],\n",
    "    hidden_size=config[\"hidden_size\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    transformer_config=transformer_config,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 03:31:54.432863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 03:31:54.432935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 03:31:54.432945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from vit_utils import load_pretrained\n",
    "jax.local_devices() # shows available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained S_16 checkpoint\n",
    "checkpoint_file = \"S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz\"\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.system(\"mkdir checkpoints\")\n",
    "os.chdir(\"checkpoints\")\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    os.system(f\"wget https://storage.googleapis.com/vit_models/augreg/{checkpoint_file}\")\n",
    "os.chdir(\"..\")\n",
    "with open(f\"checkpoints/{checkpoint_file}\", \"rb\") as f:\n",
    "    ckpt_dict = np.load(f, allow_pickle=False)\n",
    "    keys, values = zip(*list(ckpt_dict.items()))\n",
    "\n",
    "# Initialize the model with random parameters\n",
    "variables = jax.jit(lambda: model.init(\n",
    "    jax.random.PRNGKey(0),\n",
    "    # Discard the \"num_local_devices\" dimension of the batch for initialization.\n",
    "    #batch['image'][0, :1],\n",
    "    np.random.randn(2, 224, 224, 3),\n",
    "    train=False,\n",
    "), backend='cpu')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained: resized variant: %s to %s (1, 197, 384) (1, 197, 768)\n",
      "load_pretrained: grid-size from %s to %s 14 14\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "params = load_pretrained(\n",
    "    pretrained_path=f'checkpoints/{checkpoint_file}',\n",
    "    init_params=variables['params'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cs182')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ab0e2aa881ca40358338efa0b4935a3094aa552aa6f563c2ae8f08225d88408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
